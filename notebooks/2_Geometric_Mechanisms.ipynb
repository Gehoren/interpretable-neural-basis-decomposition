{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Geometry of Thought: Mechanistic Interpretability\n",
    "\n",
    "**Objective:** Visualize how neural networks construct complex functions from simple pieces.\n",
    "\n",
    "Here we shift from *optimizing* loss to *interpreting* structure. We will:\n",
    "1. **Decompose a Neural Network:** See the individual ReLU \"basis functions\" that sum up to approximate a Sine wave.\n",
    "2. **Analyze Topology:** Compare how \"Width\" (Polynomials) vs \"Depth\" (Layers) solve the Spiral Classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src.torch_engine import ExplainableReLUNet, SpiralClassifier\n",
    "from src.data_loader import generate_sine_wave, generate_spiral_data, PolynomialFeatureExpander\n",
    "from src.visualization import plot_basis_mechanisms, plot_decision_boundary\n",
    "from src.utils import set_seed\n",
    "\n",
    "set_seed(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Sum of ReLUs\n",
    "\n",
    "The Universal Approximation Theorem states that a hidden layer of sufficient width can approximate any continuous function. But *how*?\n",
    "\n",
    "We can write the output of a network as:\n",
    "$$ f(x) = \\sum_{i=1}^{N} w_{out}^{(i)} \\cdot \\text{ReLU}(w_{in}^{(i)} x + b^{(i)}) $$\n",
    "\n",
    "Each neuron learns a **Scale** ($w_{out}$), a **Slope** ($w_{in}$), and a **Kink** (via $b$). Let's visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data\n",
    "x_np, y_np = generate_sine_wave(n_samples=200)\n",
    "x_train = torch.FloatTensor(x_np)\n",
    "y_train = torch.FloatTensor(y_np).reshape(-1, 1)\n",
    "\n",
    "# 2. Model (ExplainableReLUNet)\n",
    "basis_count = 15\n",
    "model = ExplainableReLUNet(hidden_dim=basis_count)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.02)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 3. Train\n",
    "print(f\"Training with {basis_count} basis functions...\")\n",
    "for epoch in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss {loss.item():.5f}\")\n",
    "\n",
    "# 4. Visualize Decomposition\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_plot_np = np.linspace(0, 2*np.pi, 500).reshape(-1, 1)\n",
    "    x_plot = torch.FloatTensor(x_plot_np)\n",
    "    \n",
    "    # Get component-wise breakdown\n",
    "    final_pred, contributions = model.forward_decomposed(x_plot)\n",
    "\n",
    "plot_basis_mechanisms(\n",
    "    x_plot_np, \n",
    "    contributions.numpy(), \n",
    "    final_pred.numpy(), \n",
    "    np.sin(x_plot_np)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "In the plot above:\n",
    "1. **Top Panel:** The individual lines are the outputs of specific hidden neurons. Notice they are simple \"bent lines\" (ReLUs).\n",
    "2. **Bottom Panel:** The red line is the *sum* of the ghost lines above. It reconstructs the sine wave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Depth vs. Width (The Spiral)\n",
    "\n",
    "The Spiral dataset is notoriously hard for linear classifiers because it is not linearly separable. We have two ways to solve this:\n",
    "1. **Width (Feature Engineering):** Expand input space with polynomials ($x^2, xy, y^2...$).\n",
    "2. **Depth (Representation Learning):** Use layers to \"fold\" the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Data\n",
    "X_spiral, y_spiral = generate_spiral_data(n_points=1000, K=3, sigma=0.2)\n",
    "X_tensor = torch.FloatTensor(X_spiral)\n",
    "y_tensor = torch.LongTensor(y_spiral)\n",
    "\n",
    "print(\"Data Shape:\", X_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment A: The Wide Approach (Polynomials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Expand Features\n",
    "poly = PolynomialFeatureExpander(degree=5)\n",
    "X_poly = poly.transform(X_tensor)\n",
    "input_dim = X_poly.shape[1]\n",
    "print(f\"Expanded 2D -> {input_dim}D features\")\n",
    "\n",
    "# 2. Linear Classifier on High-Dim Features\n",
    "poly_model = nn.Linear(input_dim, 3)\n",
    "optimizer = optim.Adam(poly_model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 3. Train\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    out = poly_model(X_poly)\n",
    "    loss = criterion(out, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# 4. Viz Helper for Polynomials\n",
    "class PolyWrapper(nn.Module):\n",
    "    def __init__(self, model, expander):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.expander = expander\n",
    "    def forward(self, x):\n",
    "        x_poly = self.expander.transform(x)\n",
    "        return self.model(x_poly)\n",
    "\n",
    "print(\"Wide Model Decision Boundary:\")\n",
    "plot_decision_boundary(PolyWrapper(poly_model, poly), X_spiral, y_spiral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment B: The Deep Approach (MLP)\n",
    "Notice how the Deep model creates a smoother, more generalized boundary without needing manual feature math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Deep Model\n",
    "deep_model = SpiralClassifier(input_dim=2, hidden_dims=[64, 32], output_dim=3)\n",
    "optimizer = optim.Adam(deep_model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 2. Train\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    out = deep_model(X_tensor)\n",
    "    loss = criterion(out, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Deep Model Decision Boundary:\")\n",
    "plot_decision_boundary(deep_model, X_spiral, y_spiral)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
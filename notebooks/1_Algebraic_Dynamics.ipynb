{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Engine Room: Algebraic Dynamics\n",
    "\n",
    "**Objective:** Validate the \"From Scratch\" NumPy engine and explore the mathematics of optimization.\n",
    "\n",
    "In this notebook, we will:\n",
    "1. **Verify Backpropagation:** Perform a Numerical Gradient Check to prove our calculus is correct.\n",
    "2. **Compare Optimizers:** Pit SGD, Momentum, and Adam against each other on the MNIST dataset.\n",
    "3. **Visualize Convergence:** See how adaptive learning rates (Adam) speed up the descent down the loss surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Add parent directory to path to import src\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src import numpy_engine as nn\n",
    "from src.utils import set_seed\n",
    "from src.data_loader import load_mnist\n",
    "from src.visualization import plot_training_curves\n",
    "\n",
    "# Ensure reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Litmus Test (Numerical Gradient Checking)\n",
    "\n",
    "Before training, we must prove our `backward()` methods are correct. We do this by comparing the **Analytical Gradient** (calculated by chain rule in our code) vs. the **Numerical Gradient** (calculated by finite differences).\n",
    "\n",
    "$$ \\frac{df}{dx} \\approx \\frac{f(x + \\epsilon) - f(x - \\epsilon)}{2\\epsilon} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient_check(layer, x, epsilon=1e-5):\n",
    "    \"\"\"Approximates the gradient using finite differences.\"\"\"\n",
    "    # 1. Forward pass to get initial state\n",
    "    _ = layer.forward(x)\n",
    "    \n",
    "    # 2. Backward pass to get Analytical Gradient\n",
    "    # We simulate a gradient of 1.0 coming from the next layer for simplicity\n",
    "    grad_output = np.ones_like(layer.forward(x))\n",
    "    analytical_grad = layer.backward(grad_output)\n",
    "    \n",
    "    # 3. Compute Numerical Gradient for input x\n",
    "    numerical_grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        orig_val = x[idx]\n",
    "        \n",
    "        # f(x + eps)\n",
    "        x[idx] = orig_val + epsilon\n",
    "        pos_out = np.sum(layer.forward(x))\n",
    "        \n",
    "        # f(x - eps)\n",
    "        x[idx] = orig_val - epsilon\n",
    "        neg_out = np.sum(layer.forward(x))\n",
    "        \n",
    "        # Central Difference\n",
    "        numerical_grad[idx] = (pos_out - neg_out) / (2 * epsilon)\n",
    "        \n",
    "        # Reset\n",
    "        x[idx] = orig_val\n",
    "        it.iternext()\n",
    "        \n",
    "    # Compare\n",
    "    diff = np.linalg.norm(analytical_grad - numerical_grad) / (np.linalg.norm(analytical_grad) + np.linalg.norm(numerical_grad))\n",
    "    return diff\n",
    "\n",
    "# Test on a Linear Layer\n",
    "x_dummy = np.random.randn(5, 10) # Batch 5, Features 10\n",
    "layer = nn.Linear(10, 5)\n",
    "\n",
    "diff = numerical_gradient_check(layer, x_dummy)\n",
    "print(f\"Gradient Check Difference (Linear): {diff:.2e}\")\n",
    "\n",
    "assert diff < 1e-7, \"Gradient check failed! Check your math.\"\n",
    "print(\"\\u2705 Gradient Check Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Great Race (SGD vs Adam)\n",
    "\n",
    "We will train a simple MLP on a subset of MNIST to visualize how different optimizers navigate the loss landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load small subset of MNIST for speed\n",
    "x_train, y_train, x_test, y_test = load_mnist(train_size=2000, test_size=500)\n",
    "\n",
    "def build_mnist_model():\n",
    "    model = nn.NumpyMLP()\n",
    "    model.add(nn.Linear(784, 64))\n",
    "    model.add(nn.ReLU())\n",
    "    model.add(nn.Linear(64, 32))\n",
    "    model.add(nn.ReLU())\n",
    "    model.add(nn.Linear(32, 10))\n",
    "    return model\n",
    "\n",
    "def train(model, optimizer, epochs=15):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    loss_history = []\n",
    "    \n",
    "    batch_size = 64\n",
    "    num_batches = x_train.shape[0] // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        # Shuffle\n",
    "        perm = np.random.permutation(x_train.shape[0])\n",
    "        x_shuf = x_train[perm]\n",
    "        y_shuf = y_train[perm]\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            \n",
    "            # 1. Forward\n",
    "            preds = model.forward(x_shuf[start:end])\n",
    "            \n",
    "            # 2. Loss\n",
    "            loss = loss_fn.forward(preds, y_shuf[start:end])\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            # 3. Backward\n",
    "            grad = loss_fn.backward()\n",
    "            model.backward(grad)\n",
    "            \n",
    "            # 4. Optimize\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        loss_history.append(epoch_loss / num_batches)\n",
    "        print(f\"Epoch {epoch+1}: Loss {epoch_loss/num_batches:.4f}\", end='\\r')\n",
    "    \n",
    "    print(f\"\\nDone. Final Loss: {loss_history[-1]:.4f}\")\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SGD\n",
    "print(\"Training SGD...\")\n",
    "model_sgd = build_mnist_model()\n",
    "opt_sgd = nn.SGD(model_sgd.parameters(), lr=0.01)\n",
    "loss_sgd = train(model_sgd, opt_sgd)\n",
    "\n",
    "# 2. Momentum\n",
    "print(\"Training Momentum...\")\n",
    "model_mom = build_mnist_model()\n",
    "opt_mom = nn.Momentum(model_mom.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_mom = train(model_mom, opt_mom)\n",
    "\n",
    "# 3. Adam\n",
    "print(\"Training Adam...\")\n",
    "model_adam = build_mnist_model()\n",
    "opt_adam = nn.Adam(model_adam.parameters(), lr=0.001)\n",
    "loss_adam = train(model_adam, opt_adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Visualization\n",
    "\n",
    "Notice how Adam converges significantly faster in the early epochs. This is due to the **Adaptive Moment Estimation**, which scales the learning rate individually for each parameter based on the variance of its gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    'SGD': loss_sgd,\n",
    "    'Momentum': loss_mom,\n",
    "    'Adam': loss_adam\n",
    "}\n",
    "\n",
    "plot_training_curves(history, title=\"Optimizer Benchmark (NumPy Engine)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}